---
author: 胡琛
title: 支持向量机
...

概述
====

支持向量机 (support vector machines, `SVM`)
是一种二类分类模型，基本模型是定义在特征空间上的间隔最大
的线性分类器。支持向量机还包括核技巧，使其称为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化
，可形式化为一个求解凸二次规划 (convex quadratic programming)
的问题，也等价于正则化的合页损失函数的
最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。

支持向量机学习方法包括构建由简至繁的模型：
1.  线性可分支持向量机 (linear support vector machine in linearly
    separable case)：训练数据可分时，通 过硬间隔最大化 (hard margin
    maximization)，学习一个线性的分类器。

2.  线性支持向量机 (linear support vector
    machine)：训练数据近似线性可分时，通过软间隔最大化 (soft margin
    maximization)，也学习一个线性分类器

3.  非线性支持向量机 (non-linear support vector
    machine)：当训练数据不可分时，通过使用核技巧 (kernel trick)
    以及软间隔最大化，学习非线性支持向量机

当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数 (kernel
function) 表示将输入从输入
空间映射到特征空间，得到的特征向量之间的内积。通过使用核函数，可以学习非线性支持向量机，等价于隐式地
在高维的特征空间中学习线性支持向量机。这样的方法被称为核技巧。核方法
(kernel method) 是比支持向量机 更为一般的机器学习方法。

线性可分支持向量机与硬间隔最大化
================================

线性可分向量机
--------------

### 数据集

假设给定一个特征空间上的训练数据集

其中，

$x_i$ 为第 $i$ 个特征向量，也称为实例，$y_i$ 为 $x_i$ 的类标记，当
$y_i = +1$ 时，称 $x_i$ 为正例； 当 $y_i = -1$ 时，称 $x_i$ 为负例。
$(x_i, y_i)$ 为样本点，再假设训练数据集是线性可分的。

### 目标

学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程
$\omega \cdot x + b = 0$ ，它由法向量 $\omega$ 与截距 $b$ 决定的，可用
$(\omega, b)$ 表示。分离超平
面将特征空间划分为两部分，一部分是正类，一部分是负类，法向量指向的一侧是正类，另一侧是负类。

### 说明

一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开，感知机利用误分类最小的策略
，求得分离超平面，不过，此时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，此时，
解是唯一的。

### 定义

给定线性可分[^1]训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为：

以及相应的分类决策函数：

称为线性可分支持向量机。

### 举例

如图 <fig:TwoKindsDiv>
所示，红色点与蓝色点就是线性可分的，我们需要做的是找到一个最优化的分割方案。线性可分支持向量机
就是去找到一种方案，使得分割线与两边的间隔最大。

![二类分类问题](file:./figs/supvector_01.png "TwoKindsDiv")

函数间隔与几何间隔
------------------

### 说明

对于数据集 $T = \{(x_1,y_1), (x_2,y_2), \ldots, (x_N,y_N)\}$
上的数据点，它到分隔超平面的距离肯定是 关于坐标 $(\vec{x}, \vec{y})$
的一个函数，考虑到几何上点到面的距离向量必然与超平面的法向量平行，假设
超平面的法向量为 $\vec{\omega}$
，同时，由于需要考虑平面两侧不同，点到面的距离我们可以人为设置正负，
以便与分类标签 $y_i = {+1, -1}$
对应，于是，很自然地我们可以写出点到面的距离的函数如下：

其中， $y_i$ 是数据点的 label，按数据点在分界面的哪一边来定， $b$
是截距。以上的方式有一个缺点， 当我们对上式中 $\omega$ 和 $b$
同时乘以一个因子时，由此确定的超平面是不变的，但是由上式定义出的函数
间隔却变为原来的两倍。这意味着，仅靠上式来确定数据点到分离面的距离是不够的。为此，我们可以通过几何上
点到面的距离计算公式，来确定数据点到分离面的距离，如图 <fig:geointerval>
所示的 $\gamma_i$ ，几何上可以 由下式给出：

![几何间隔示意图](file:./figs/supvector_02.jpg "geointerval")

然后，我们只需要找到 $\gamma = \min \limits_{i=1,\ldots,N}\gamma_i$
并使之取极大值，就可以确定我们需要的分隔
超平面与数据点到分离面的几何距离了。

### 定义

-   函数间隔

对于给定的训练数据集 $T$ 和 超平面 $(\omega, b)$ ，定义超平面
$(\omega, b)$ 关于样本点 $(x_i, y_i)$ 的函数间隔为：

定义超平面 $(\omega, b)$ 关于训练数据集 $T$ 的函数间隔为超平面
$(\omega,b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的函数间隔最小值，即：

-   几何间隔

对于给定的训练数据集 $T$ 和 超平面 $(\omega, b)$ ，定义超平面
$(\omega, b)$ 关于样本点 $(x_i, y_i)$ 的几何间隔为：

定义超平面 $(\omega, b)$ 关于训练数据集 $T$ 的几何间隔为超平面
$(\omega,b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的几何间隔最小值，即：

### 关系

由定义可知，函数间隔与几何间隔关系如下：

间隔最大化
----------

对于线性可分训练数据集，分离超平面有无数个，我们的想法是求出分离超平面关于训练数据集的几何间隔，使其
取最大值，以此来得到唯一的分离超平面。这里的间隔最大化又被称为硬间隔最大化。对此处理方法的直观解释：
对训练集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点
分开，而且对最难分的实例点 (离分离超平面最近的点)
也有足够大的确定度将它们分开，这样的超平面应该对未
知的新实例有很好的分类预测能力。

### 最大间隔分离超平面

对于我们的想法，用数学语言表达就是：

考虑到函数间隔与几何间隔关系，上式又可以写成，

可以看出，函数间隔的取值 $\bar{\gamma}$
并不影响最优化问题的解[^2]。因此，为了方便计算，我们可以取
$\bar{\gamma} = 1$ ，并将其带入上式，同时，考虑到最大化
$\frac{1}{||\omega||}$ 与最小化 $\frac{1}{2}||\omega||^2$
是等价的[^3]，于是，上述最优化问题转换为下面的线性可分向量机学习的最优化
问题： 上式是一个凸二次规划 (convex quadratic programming)
问题。如果求出了约束最优化问题 <eq:convquapro> 的 解
$\omega^{\ast}, b^{\ast}$ ，就可以得到最大间隔分离超平面
$\omega^{\ast}\cdot{}x+b^{\ast}$ 及分类 决策函数
$f(x) = sign(\omega^{\ast}\cdot{}x+b^{\ast}$
，即线性可分支持向量机模型。

综上，我们可以得到下面的线性可分支持向量机的学习算法--最大间隔法
(maximum margin method):

算法 1：线性可分支持向量机学习算法--最大间隔法

输入：线性可分训练数据集
$T=\{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}$ ，其中，
$x_i \in \mathcal{X} = R^n, y_i \in \mathcal{Y}={+1, -1}, i=1,2,\ldots,N$
;

输出：最大间隔分离超平面和分类决策函数。

1.  构造并求解约束最优化问题： 求得最优解 $\omega^{\ast}, b^{\ast}$

2.  由此得到分离超平面 和分类决策函数

[^1]: 线性可分的定义：给定一个数据集
    $T = \{(x_1,y_1), (x_2,y_2), \ldots, (x_N,y_N)\}$ ，其中，
    $x_i \in \mathcal{X} = {\bm R}^n, y_i \in \mathcal{Y} = \{+1, -1\}, i=1,2,\ldots,N$
    ，如果存在某个超 平面 S： $\omega \cdot x + b = 0$
    能够将数据集完全正确地划分到超平面的两侧，即对所有 $y_i = +1$ 的实
    例 $i$ ，有 $\omega \cdot x + b > 0$ ，对所有 $y_i = -1$ 的实例 $i$
    ，有 $\omega \cdot x + b < 0$ ， 则称数据集 T 是线性可分数据集
    (linear separable data set)；否则，称数据集为线性不可分。

[^2]: 事实上，如果考虑到拉格朗日乘子法的时候，这一点可以更加明显地表现出来。

[^3]: 最大化 $\frac{1}{|x|}$ 等价于最小化 $|x|$ 等价于最小化
    $\frac{1}{2}|x|^2$ 。
