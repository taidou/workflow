#+TITLE: 向量机
#+AUTHOR: 胡琛

* 概述
支持向量机 (support vector machines, =SVM=) 是一种二类分类模型，基本模型是定义在特征空间上的间隔最大
的线性分类器。支持向量机还包括核技巧，使其称为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化
，可形式化为一个求解凸二次规划 (convex quadratic programming) 的问题，也等价于正则化的合页损失函数的
最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。

支持向量机学习方法包括构建由简至繁的模型：
1. 线性可分支持向量机 (linear support vector machine in linearly separable case)：训练数据可分时，通
   过硬间隔最大化 (hard margin maximization)，学习一个线性的分类器。

2. 线性支持向量机 (linear support vector machine)：训练数据近似线性可分时，通过软间隔最大化
   (soft margin maximization)，也学习一个线性分类器

3. 非线性支持向量机 (non-linear support vector machine)：当训练数据不可分时，通过使用核技巧
   (kernel trick) 以及软间隔最大化，学习非线性支持向量机

当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数 (kernel function) 表示将输入从输入
空间映射到特征空间，得到的特征向量之间的内积。通过使用核函数，可以学习非线性支持向量机，等价于隐式地
在高维的特征空间中学习线性支持向量机。这样的方法被称为核技巧。核方法 (kernel method) 是比支持向量机
更为一般的机器学习方法。

* 线性可分支持向量机与硬间隔最大化

** 线性可分向量机
*** 数据集
假设给定一个特征空间上的训练数据集
  \begin{equation}
    T = \{(x_1,y_1), (x_2,y_2), \ldots, (x_N,y_N)\},
  \end{equation}
其中，
  \begin{equation}
    x_i \in \mathcal{X} = {\bm R}^n, y_i \in \mathcal{Y} = \{+1, -1\}, i=1,2,\ldots,N,
  \end{equation}
$x_i$ 为第 $i$ 个特征向量，也称为实例，$y_i$ 为 $x_i$ 的类标记，当 $y_i = +1$ 时，称 $x_i$ 为正例；
当 $y_i = -1$ 时，称 $x_i$ 为负例。 $(x_i, y_i)$ 为样本点，再假设训练数据集是线性可分的。

*** 目标
学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程
$\omega \cdot x + b = 0$ ，它由法向量 $\omega$ 与截距 $b$ 决定的，可用 $(\omega, b)$ 表示。分离超平
面将特征空间划分为两部分，一部分是正类，一部分是负类，法向量指向的一侧是正类，另一侧是负类。

*** 说明
一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开，感知机利用误分类最小的策略
，求得分离超平面，不过，此时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，此时，
解是唯一的。

*** 定义
给定线性可分[fn:1]训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为：
\begin{equation}
\omega^{\ast} \cdot x + b^{\ast} = 0
\end{equation}
以及相应的分类决策函数：
  \begin{equation}
    f(x) = sign(\omega^{\ast} \cdot x + b^{\ast})
  \end{equation}
称为线性可分支持向量机。

*** 举例
如图 [[fig:TwoKindsDiv]] 所示，红色点与蓝色点就是线性可分的，我们需要做的是找到一个最优化的分割方案。线性可分支持向量机
就是去找到一种方案，使得分割线与两边的间隔最大。

#+CAPTION: 二类分类问题
#+NAME: fig:TwoKindsDiv
#+ATTR_LATEX: :width 5cm
[[./figs/supvector_01.png]]

** 函数间隔与几何间隔

*** 说明
对于数据集 $T = \{(x_1,y_1), (x_2,y_2), \ldots, (x_N,y_N)\}$ 上的数据点，它到分隔超平面的距离肯定是
关于坐标 $(\vec{x}, \vec{y})$ 的一个函数，考虑到几何上点到面的距离向量必然与超平面的法向量平行，假设
超平面的法向量为 $\vec{\omega}$ ，同时，由于需要考虑平面两侧不同，点到面的距离我们可以人为设置正负，
以便与分类标签 $y_i = {+1, -1}$ 对应，于是，很自然地我们可以写出点到面的距离的函数如下：
#+NAME: eq:funcinterval
  \begin{equation}
    \bar{\gamma}_i = y_i(\omega \cdot x_i + b)
  \end{equation}
其中， $y_i$ 是数据点的 label，按数据点在分界面的哪一边来定， $b$ 是截距。以上的方式有一个缺点，
当我们对上式中 $\omega$ 和 $b$ 同时乘以一个因子时，由此确定的超平面是不变的，但是由上式定义出的函数
间隔却变为原来的两倍。这意味着，仅靠上式来确定数据点到分离面的距离是不够的。为此，我们可以通过几何上
点到面的距离计算公式，来确定数据点到分离面的距离，如图 [[fig:geointerval]] 所示的 $\gamma_i$ ，几何上可以
由下式给出：
  \begin{equation}
    \gamma_i = y_i\left(\frac{\omega}{||\omega||} \cdot x_i + \frac{b}{||\omega||}\right)
  \end{equation}

#+CAPTION: 几何间隔示意图
#+NAME: fig:geointerval
#+ATTR_LATEX: :width 7cm
[[./figs/supvector_02.jpg]]

然后，我们只需要找到 $\gamma = \min \limits_{i=1,\ldots,N}\gamma_i$ 并使之取极大值，就可以确定我们需要的分隔
超平面与数据点到分离面的几何距离了。

*** 定义
+ 函数间隔

对于给定的训练数据集 $T$ 和 超平面 $(\omega, b)$ ，定义超平面 $(\omega, b)$ 关于样本点 $(x_i, y_i)$
的函数间隔为：
  \begin{equation}
    \bar{\gamma_i} = y_i(\omega \cdot x_i + b)
  \end{equation}

定义超平面 $(\omega, b)$ 关于训练数据集 $T$ 的函数间隔为超平面 $(\omega,b)$ 关于 $T$ 中所有样本点
$(x_i, y_i)$ 的函数间隔最小值，即：
  \begin{equation}
    \bar{\gamma} = \min_{i=1,\ldots,N}\bar{\gamma_i}
  \end{equation}

+ 几何间隔

对于给定的训练数据集 $T$ 和 超平面 $(\omega, b)$ ，定义超平面 $(\omega, b)$ 关于样本点 $(x_i, y_i)$
的几何间隔为：
  \begin{equation}
    \gamma_i = y_i\left(\frac{\omega}{||\omega||} \cdot x_i + \frac{b}{||\omega||}\right)
  \end{equation}

定义超平面 $(\omega, b)$ 关于训练数据集 $T$ 的几何间隔为超平面 $(\omega,b)$ 关于 $T$ 中所有样本点
$(x_i, y_i)$ 的几何间隔最小值，即：
  \begin{equation}
    \gamma = \min_{i=1,\ldots,N}\gamma_i
  \end{equation}

*** 关系
由定义可知，函数间隔与几何间隔关系如下：
  \begin{eqnarray}
    \gamma_i &=& \frac{\bar{\gamma_i}}{||\omega||}\\
    \gamma &=& \frac{\bar{\gamma}}{||\omega||}
  \end{eqnarray}

** 间隔最大化
对于线性可分训练数据集，分离超平面有无数个，我们的想法是求出分离超平面关于训练数据集的几何间隔，使其
取最大值，以此来得到唯一的分离超平面。这里的间隔最大化又被称为硬间隔最大化。对此处理方法的直观解释：
对训练集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点
分开，而且对最难分的实例点 (离分离超平面最近的点) 也有足够大的确定度将它们分开，这样的超平面应该对未
知的新实例有很好的分类预测能力。

*** 最大间隔分离超平面
对于我们的想法，用数学语言表达就是：

  \begin{eqnarray}
    \max_{\omega, b} && \gamma\\
    s.t. && y_i\left( \frac{\omega}{||\omega||} \cdot x_i + \frac{b}{||\omega||} \right) \geq \gamma,
            i = 1,2,\ldots,N
  \end{eqnarray}

考虑到函数间隔与几何间隔关系，上式又可以写成，

  \begin{eqnarray}
    \max_{\omega, b} && \bar{\gamma}\\
    s.t. && y_i (\omega \cdot x_i + b) \geq \bar{\gamma}, i = 1,2,\ldots,N
  \end{eqnarray}

可以看出，函数间隔的取值 $\bar{\gamma}$ 并不影响最优化问题的解[fn:2]。因此，为了方便计算，我们可以取
$\bar{\gamma} = 1$ ，并将其带入上式，同时，考虑到最大化 $\frac{1}{||\omega||}$ 与最小化
$\frac{1}{2}||\omega||^2$ 是等价的[fn:3]，于是，上述最优化问题转换为下面的线性可分向量机学习的最优化
问题：
#+NAME: eq:convquapro
  \begin{eqnarray}
    \min_{\omega, b} && \frac{1}{2}||\omega||^2\\
    s.t. && y_i (\omega \cdot x_i + b) -1 \geq 0, i = 1,2,\ldots,N
  \end{eqnarray}
上式是一个凸二次规划 (convex quadratic programming) 问题。如果求出了约束最优化问题 [[eq:convquapro]] 的
解 $\omega^{\ast}, b^{\ast}$ ，就可以得到最大间隔分离超平面 $\omega^{\ast}\cdot{}x+b^{\ast}$ 及分类
决策函数 $f(x) = sign(\omega^{\ast}\cdot{}x+b^{\ast}$ ，即线性可分支持向量机模型。

综上，我们可以得到下面的线性可分支持向量机的学习算法--最大间隔法 (maximum margin method):

算法 1：线性可分支持向量机学习算法--最大间隔法

输入：线性可分训练数据集 $T=\{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}$ ，其中，
$x_i \in \mathcal{X} = R^n, y_i \in \mathcal{Y}={+1, -1}, i=1,2,\ldots,N$ ;

输出：最大间隔分离超平面和分类决策函数。

1) 构造并求解约束最优化问题：

   #+NAME: eq:originquestion
     \begin{eqnarray}
       \min_{\omega, b} && \frac{1}{2}||\omega||^2\\\nonumber
       s.t. && y_i (\omega \cdot x_i + b) -1 \geq 0, i = 1,2,\ldots,N
     \end{eqnarray}
   求得最优解 $\omega^{\ast}, b^{\ast}$

3) 由此得到分离超平面
     \begin{equation}
       \omega^{\ast} \cdot x + b^{\ast} = 0
     \end{equation}
   和分类决策函数
     \begin{equation}
       f(x) = sign(\omega^{\ast}\cdot{}x+b^{\ast})
     \end{equation}

4) 可以证明，最大间隔分离超平面存在且唯一

*** 支持向量与间隔边界
1. 支持向量

   在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。即使得最优化条
   件中不等式等号成立的点，
   \begin{equation}
   y_i(\omega \cdot x_i + b) - 1 = 0
   \end{equation}

   对于 $y_i = +1$ 的正例点，支持向量在超平面 $H1: \omega \cdot x + b = 1$ ；对于 $y_i = -1$ 的负例点，
   支持向量在超平面 $H2: \omega \cdot x + b = -1$ 。

3. 间隔边界

   如下图所示，粉色线为 $H1$ ，蓝色线为 $H2$ ，两者之间的距离，称为间隔。分离超平面位于两者中央与两者平
   行。间隔依赖于分离超平面的法向量 $\omega$ ，等于 $\frac{2}{||\omega||}$ ， $H1,H2$ 称为间隔边界。

4. 说明

   在决定分离超平面时，只有支持向量起作用，其他实例点并不起作用，移动或添加其他实例点并不影响我们的求解
   。由于支持向量再确定分离超平面中起着决定性作用，所以将这类分类模型称为支持向量机。支持向量的个数很少
   ，所以支持向量机由很少的 “重要的” 训练样本决定。

5. 举例

   已知训练数据集，其正例点为 $x_1=(3,3)^{T}$ ， $x_2=(4,3)^{T}$ ，负例点为 $x_3=(1,1)^{T}$ ，试求最
   大间隔分离超平面。

   解：
   1. 构造数据集约束最优化问题：
        \begin{eqnarray}
            \min_{\omega, b}: &&\frac{1}{2}(\omega^{2}_{1}+\omega^{2}_{2})\\\nonumber
            s.t. & & 3\omega_1+3\omega_2+b \geq 1\\\nonumber
            & & 4\omega_1+3\omega_2+b \geq 1\\\nonumber
            & & -\omega_1-\omega_2-b \geq 1
        \end{eqnarray}
   2. 解最优化问题，

      实际上，如果在坐标轴上将三个点画出，可以很容易找到最优解为穿过点 $(0, 4)$ 和 $(4, 0)$ 的直线，
      利用两个支持向量所在超平面对应的等式 $3\omega_1+3\omega_2+b=1$ 和 $-\omega_1-\omega_2-b=1$ 可
      以求出 $b=-2$ ，直线的已知，可以很容易看出 $\omega_1 = \omega_2 = \frac{1}{2}$ 。于是，我们最
      终得到最大间隔分离超平面为：
        \begin{equation}
          \frac{1}{2}x^{(1)}+\frac{1}{2}x^{(2)}-2 = 0
        \end{equation}
      其中， $x_1 = (3,3)^T$ 与 $x_2 = (1,1)^T$ 为支持向量。

*** 学习的对偶算法
1. 拉格朗日对偶性

   1) 原始问题

      假设 $f(x)$ ， $c_i(x)$ ， $h_j(x)$ 是定义在 ${\rm R}^n$ 上的连续可微函数，考虑约束最优化问题
        \begin{eqnarray}
            \min_{x\in{}{\rm R}^n} & &f(x)\\\nonumber
            s.t. & &c_i(x) \leq 0, i= 1,2,\ldots, k\\\nonumber
            & &h_j(x) = 0, j = 1,2,\ldots, l
        \end{eqnarray}
      称此问题为原始最优化问题或者原始问题。

      引入广义拉格朗日函数 (generalized Lagrange function)
        \begin{equation}
          L(x, \alpha, \beta) = f(x) + \sum^{k}_{i=1}\alpha_ic_i(x) +
          \sum^{l}_{j=1}\beta_jh_j(x)
        \end{equation}
      这里， $x=(x^{(1)}, x^{(2)}, \ldots, x^{(n)})^T\in{}{\rm R}^n$ ， $\alpha_i, \beta_j$ 是拉格朗
      日乘子， $\alpha_i\geq{}0$ 。考虑 $x$ 的函数：
        \begin{equation}
          \theta_P(x) = \max_{\alpha,\beta:\alpha_i\geq{}0} L(x,\alpha,\beta)
        \end{equation}
      这里下标 $P$ 表示原始问题。对于某个给定的 $x$ ，如果 $x$ 违反原始问题的约束条件，即存在某个
      $i$ 使得 $c_i(\omega) > 0$ 或存在某个 $j$ 使得 $h_j(\omega) \neq 0$ ，那么就有：
        \begin{equation}
          \theta_P(x) = \max_{\alpha,\beta:\alpha_i\geq{}0}\left[ f(x) + \sum^{k}_{i=1}\alpha_ic_i(x) +
          \sum^{l}_{j=1}\beta_jh_j(x)\right] = +\infty
        \end{equation}
      因为若某个 $i$ 使约束 $c_i(x) > 0$ ，则可令 $\alpha_i\sim{}+\infty$ ；若某个 $j$ 使得
      $h_j(x)\neq{}0$ ，总可令 $\beta_jh_j(x) \sim +infty$ ，而将其他 $\alpha_i,\beta_j$ 取为 0。
      因此，如果 $x$ 满足约束条件，那么就有：
      \begin{equation}
          \theta_p(x) =
        \begin{cases}
          f(x) & x \text{满足原始问题约束}\\
          +\infty & \text{其他}
        \end{cases}
        \end{equation}

      于是，我们如果考虑极小化问题
        \begin{equation}
          \min_{x}\theta_P(x) = \min_{x} \max_{\alpha,\beta:\alpha\geq{}0}L(x, \alpha, \beta)
        \end{equation}
      它是与原始问题等价的问题，这样，我们就可以通过求解上式广义拉格朗日的极小极大问题来求解原始约束
      最优化问题的解。
   2) 对偶问题

      定义 $\theta_D(x) = \min_{x} L(x, \alpha, \beta)$ ，然后考虑极大化 $\theta_D(x)$ ，即：
        \begin{equation}
          \max_{\alpha, \beta: \alpha_i\geq{}0}\theta_D(\alpha, \beta) =
          \max_{\alpha, \beta: \alpha_i\geq{}0} \min_{x} L(x, \alpha, \beta)
        \end{equation}
      问题 $\max_{\alpha, \beta: \alpha_i\geq{}0} \min_{x} L(x, \alpha, \beta)$ 称为广义拉格朗日函数
      的极大极小问题。

      可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：
        \begin{eqnarray}
          \max_{\alpha,\beta} \theta_D(\alpha, \beta) &=& \max_{\alpha,\beta} \min_{x}L(x,\alpha,\beta)\\
          \nonumber
          s.t. && \alpha_i \geq 0, i = 1,2,\ldots,k
        \end{eqnarray}
      称为原始问题的对偶问题，定义对偶问题的最优值
          \begin{equation}
            d^{\ast} = \max_{\alpha,\beta:\alpha_i\geq{}0}\theta_D(\alpha,\beta)
          \end{equation}
      称为对偶问题的值。
   3) 原始问题与对偶问题的关系

      定理 1：若原始问题和对偶问题都有最优值，则
        \begin{equation}
          d^{\ast} = \max_{\alpha,\beta:\alpha_i\geq{}0}\min_{x}L(x,\alpha,\beta) \leq
          \min_{x}\max_{\alpha,\beta:\alpha_i\geq{}0}L(x,\alpha,\beta) = p^{\ast}
        \end{equation}

      推论 1：设 $x^{\ast}$ 和 $\alpha^{\ast}, \beta^{\ast}$ 分别是原始问题和对偶问题的可行解，而且有
      $d^{\ast} = p^{\ast}$ ，则 $x^{\ast}$ 和 $\alpha^{\ast}, \beta^{\ast}$ 分别是原始问题和对偶问
      题的最优解。

      定理 2：考虑原始问题与对偶问题，假设 $f(x)$ 和 $c_i(x)$ 是凸函数[fn:4]，
      $h_j(x)$ 是仿射函数[fn:5]；并且假设不等式约束 $c_i(x)$ 是严格可行的，即存在 $x$ ，对所有 $i$
      有 $c_i(x) < 0$ ，则存在 $x^{\ast}, \alpha^{\ast}, \beta^{\ast}$ ，使得 $x^{\ast}$ 是原始问题
      的解， $\alpha^{\ast}, \beta^{\ast}$ 是对偶问题的解，而且
        \begin{equation}
          p^{\ast} = d^{\ast} = L(x^{\ast}, \alpha^{\ast}, \beta^{\ast})
        \end{equation}

     定理 3：对原始问题和对偶问题，假设 $f(x)$ 和 $c_i(x)$ 是凸函数， $h_j(x)$ 是仿射函数，并且不等式
     约束 $c_i(x)$ 是严格可行的，则 $x^{\ast}, \alpha^{\ast}, \beta^{\ast}$ 分别是原始问题和对偶问题
     的解的充分必要条件是 $x^{\ast}, \alpha^{\ast}, \beta^{\ast}$ 必须满足下面的 KKT 条件：
     #+NAME: eq:dualbestques
       \begin{eqnarray}
         \nabla_{x}L(x^{\ast}, \alpha^{\ast}, \beta^{\ast}) & = & 0\\\nonumber
         \nabla_{\alpha}L(x^{\ast}, \alpha^{\ast}, \beta^{\ast}) & = & 0\\\nonumber
         \nabla_{\beta}L(x^{\ast}, \alpha^{\ast}, \beta^{\ast}) & = & 0\\\nonumber
         \alpha^{\ast}c_i(x^{\ast}) =0,&& i = 1,2,\ldots,k\\\nonumber
         c_i(x^{\ast}) \leq 0,&& i = 1,2,\ldots,k\\\nonumber
         \alpha_{i}^{\ast} \geq 0 ,&& i = 1,2,\ldots,k\\\nonumber
         h_{j}(x^{\ast}) = 0 ,&& i = 1,2,\ldots,l
       \end{eqnarray}
2. 利用对偶问题求解原始问题的最优解

   1) 构建拉格朗日函数

      对原始问题 (式 [[eq:originquestion]]) 中每个不等式约束引入拉格朗日乘子 (Lagrange multiplier)
      $\alpha_i \geq 0, i=1,2,\ldots,N$ ，定义拉格朗日函数：

      #+NAME: eq:lagrangeorigin
        \begin{equation}
          L(\omega, b, a) = \frac{1}{2}||\omega||^2-\sum^{N}_{i=1}\alpha_iy_i(\omega\cdot{}x_i+b)
      + \sum^{N}_{i=1}\alpha_i
      \end{equation}
      其中， $\alpha=(\alpha_1, \alpha_2, \ldots, \alpha_N)^T$ 为拉格朗日乘子向量。

   2) 原始问题的对偶问题
      由之前讨论可知，与原始问题等价的极小极大问题是：
        \begin{equation}
        \min_{\omega,b} \max_{\alpha} L(\omega, b, \alpha)
        \end{equation}
      与之对偶的极大极小问题是：
        \begin{equation}
          \max_{\alpha}\min_{\omega,b} L(\omega, b, \alpha)
        \end{equation}

      考虑到 $\frac{1}{2}||\omega||^2$ 和 $-y_i(\omega\cdot{}x_i+b)+1$ 均为凸函数，满足定理 2 的条件，
      我们知道，原始问题与对偶问题的解是存在的。下面对原始问题的对偶问题进行转换：
      1. 求 $\min_{\omega, b} L(\omega, b, \alpha)$
           \begin{eqnarray*}
             \nabla_{\omega}L(\omega,b,\alpha) &=& \omega - \sum^{N}_{i=1}\alpha_iy_ix_i = 0\\
             \nabla_{b}L(\omega,b,\alpha) &=& \sum^{N}_{i=1}\alpha_iy_i = 0
           \end{eqnarray*}
         有：
           \begin{eqnarray}
             \omega & = & \sum^{N}_{i=1}\alpha_iy_ix_i\\\nonumber
             \sum^{N}_{i=1}\alpha_iy_i &=& 0
           \end{eqnarray}
        将上式代入拉格朗日函数 [[eq:lagrangeorigin]] 可以得到：
          \begin{align*}
            L(\omega, b, \alpha) &= \frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j
            (x_i\cdot{}x_j) + \sum^N_{i=1}\alpha_i\\\nonumber
            {} & = -\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
            +\sum^N_{i=1}\alpha_i
         \end{align*}
        即：
          \begin{equation}
            \min_{\omega,b,\alpha} =
         -\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
         +\sum^N_{i=1}\alpha_i
          \end{equation}
      2. 求 $\min_{\omega,b}L(\omega,b,\alpha)$ 对 $\alpha$ 的极大，即是对偶问题
           \begin{eqnarray}
             \max_{\laph}&&-\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
                    +\sum^N_{i=1}\alpha_i\\\nonumber
             s.t. && \sum^N_{i=1}\alpha_iy_i = 0\\\nonumber
             &&\alpha_i \geq 0, i=1,2,\ldots,N
           \end{eqnarray}
         将上式问题转为取极小，可以得到下面的等价的最优化问题：
           \begin{eqnarray}
             \min_{\laph}&&\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
                    -\sum^N_{i=1}\alpha_i\\\nonumber
             s.t. && \sum^N_{i=1}\alpha_iy_i = 0\\\nonumber
             &&\alpha_i \geq 0, i=1,2,\ldots,N
           \end{eqnarray}

   于是，设 $\alpha^{\ast} = (\alpha_{1}^{\ast}, \alpha_{2}^{\ast},\ldots,\alpha_{N}^{\ast})^T$ 是
   对偶最优化问题 [[eq:dualbestques]] 的解，由 $\alpha^{\ast}$ 可以求出原始最优化问题 [[eq:convquapro]] 的解
   $\omega^{\ast}$ 和 $b^{\ast}$ ，并且有如下定理：
   定理 4[fn:6]：设 $\alpha^{\ast}$ 是对偶最优化问题的解，则存在下标 $j$ ，使得 $\alpha_{j}^{\ast} > 0$ ，
   并可按下式求得原始最优化问题的解 $\omega^{\ast}, b^{\ast}$ ：
     \begin{eqnarray}
       \omega^{\ast} &=& \sum^{N}_{i=1}\alpha_{i}^{\ast}y_ix_i\\\nonumber
       b^{\ast} &=& y_j - \sum^{N}_{i=1}\alpha^{\ast}y_i(x_i\cdot{}x_j)
     \end{eqnarray}
3. 总结

   算法 2：线性可分支持向量机学习算法

   输入： 线性可分训练集 $T={(x_1,y_1), (x_2,y_2), \ldots, (x_N, y_N)}$ ，其中
   $x_i\in{}\mathcal{X}={\bm R}^n, y_i\in{}\mathcal{Y}={+1, -1}, i=1,2,\ldots,N$ ；

   输出：分离超平面和分类决策函数

   1) 构造并求解约束最优化问题：
        \begin{eqnarray*}
          \min_{\alpha}&&\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
                         -\sum^N_{i=1}\alpha_i\\
          s.t. && \sum^N_{i=1}\alpha_iy_i = 0\\
                      &&\alpha_i \geq 0, i=1,2,\ldots,N
        \end{eqnarray*}
      求解最优解 $\alpha^{\ast} = (\alpha^{\ast}_1,\alpha^{\ast}_2,\ldots,\alpha^{\ast}_N)^T$

   2) 计算
        \begin{equation*}
          \omega^{\ast} = \sum^{N}_{i=1}\alpha^{\ast}_iy_ix_i
        \end{equation*}
      并选择 $\alpha^{\ast}$ 的一个正分量 $\alpha^{\ast}_j > 0$ ，计算
        \begin{equation*}
          b^{\ast} = y_j - \sum^{N}_{i=1}\alpha^{\ast}_iy_i(x_i\cdot{}x_j)
        \end{equation*}

   3) 求得分离超平面
        \begin{equation*}
          \omega^{\ast}\cdot{}x+b^{\ast} = 0
        \end{equation*}
      分离决策函数：
        \begin{equation*}
          f(x) = sign(\omega^{\ast}\cdot{}x+b^{\ast})
        \end{equation*}

   4) 定义(支持向量)： 从上式可以看出， $\omega^{\ast}$ 和 $b^{\ast}$ 只依赖于训练数据中
      $\alpha^{\ast}_i > 0$ 的点，我们将训练数据中对应 $\alpha^{\ast}_i > 0$ 的实例点
      ($\alpha^{\ast}_i\in{}{\rm R}^n$)称为支持向量。

      根据这一定义，支持向量一定位于间隔边界上。因为由 KKT 互补条件：
        \begin{equation*}
          \alpha^{\ast}_i(y_i(\omega^{\ast}\cdot{}x_i+b^{\ast}) -1) = 0, i = 1,2,\ldots,N
        \end{equation*}
      可知，若 $\alpha^{\ast}_i > 0$ ，则对应实例 $x_i$ 有：
        \begin{equation*}
          y_i(\omega^{\ast}\cdot{}x_i+b)-1=0
        \end{equation*}
      或
        \begin{equation*}
          \omega^{\ast}\cdot{}x_i+b=\pm 1
        \end{equation*}
4. 举例
   已知数据集正例点为 $x_1=(3,3)^{T}$, $x_2=(4,3)^{T}$ , 负例点为 $x_3=(1,1)^{T}$ ，试求最大间隔分离超
   平面。

   解：
   1） 构造数据集约束最优化问题：
        \begin{eqnarray*}
            \min_{\alpha}: && \frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{i=1}\alpha_i\alpha_jy_iy_j
            (x_i\cdot{}x_j) - \sum^N_{i=1}\alpha_i\\\nonumber
            && = \frac{1}{2}18\alpha^2_1+25\alpha^2_2+2\alpha^2_3+42\alpha_1\alpha_2-
            12\alpha_1\alpha_3-14\alpha_2\alpha_3)
              -\alpha_1-\alpha_2-\alpha_3\\\nonumber
            s.t. && \alpha_1+\alpha_2-\alpha_3 = 0\\\nonumber
            && \alpha_i\geq 0, i = 1,2,3
        \end{eqnarray*}
   2） 解最优化问题，将 $\alpha_3=\alpha_1+\alpha2$ 带入目标函数并记为
        \begin{equation}
          s(\alpha_1,\alpha_2) = 4\alpha^2_1+\frac{13}{2}\alpha^2_2+10\alpha_1\alpha_2-2\alpha_1
          -2\alpha_2
        \end{equation}
       对 $\alpha_1, \alpha_2$ 分别求偏导并令其为 0，可以知道 $s(\alpha_1, \alpha_2)$ 在点
       $(\frac{3}{2}, -1)^T$ 取极值，但是该点的 $\alpha_2$ 不满足约束 $\alpha_i \geq 0$ ，所以最小值
       应在边界上达到：
       + 当 $\alpha_1 = 0$ 时，最小值 $s(0, \frac{2}{13}) = -\frac{2}{13}$ ；当 $\alpha_2=0$ 时，相应
         的 $s(\frac{1}{4}, 0) = -\frac{1}{4}$ ，于是，在 $\alpha_1=\frac{1}{4}, \alpha_2=0$ 时，
         $s$ 达到最小值，此时 $\alpha_3 = \frac{1}{4}$
       + $\alpha^{\ast}_1=\alpha^{\ast}_3=\frac{1}{4}$ 对应的实例点 $x_1, x_3$ 为支持向量，此时
            \begin{equation*}
              \omega^{\ast}_1=\omega^{\ast}_2=\frac{1}{2} & b^{\ast} = -2
            \end{equation*}
          于是，分离超平面为：
            \begin{equation*}
              \frac{1}{2}x^{(1)}+\frac{1}{2}x^{(2)} - 2 = 0
            \end{equation*}
          分类决策函数为
            \begin{equation*}
              f(x) = \frac{1}{2}x^{(1)}+\frac{1}{2}x^{(2)} - 2
            \end{equation*}

* 线性支持向量机与软间隔最大化

** 线性支持向量机
通常情况下，训练数据不是简单的可分数据集，而是有一些特异点 (outlier)，当将这些特异点除去后，剩下的大
部分样本点组成的集合是线性可分的。

对类似训练样本的每个样本点 $(x_i,y_i)$ 引入一个松弛变量 $\xi_i\geq{}0$ ，使函数间隔加上松弛变量大于
等于 1。这样，约束条件变为：
  \begin{equation}
    y_i(\omega\cdot{}x_i+b)\geq 1 - \xi_i
  \end{equation}
同时，对每个松弛变量 $\xi_i$ ，支付一个代价 $\xi_i$ ，目标函数由原来的 $\frac{1}{2}||\omega||^2$ 变
为
  \begin{equation}
    \frac{1}{2}||\omega||^2+C\sum^{N}_{i=1}\xi_i
  \end{equation}
这里， $C>0$ 称为惩罚参数，一般由应用问题决定， $C$ 值大的时候对误分类的惩罚增大，反之则减小。最小化
目标函数包含两层意思：使 $\frac{1}{2}||\omega||^2$ 尽可能小以使最大间隔尽可能大，同时使得误分类点的
个数尽可能少， $C$ 是调和两者的系数。

线性不可分的线性支持向量机的学习问题现在变成了凸二次规划问题：
#+NAME: eq:linearnondiv
  \begin{eqnarray}
   \min_{\omega,b,\xi} && \frac{1}{2}||\omega||^2 + C\sum^{N}_{i=1}\xi_i\\\nonumber
    s.t. && y_i(w\cdot{}x_i+b) \geq 1-\xi_i, i = 1,2,\ldots,N\\\nonumber
    && \xi_i \geq 0, i = 1,2,\ldots,N
  \end{eqnarray}
可以证明以上问题的解是存在的，而且 $\omega$ 的解是唯一的， $b$ 的解不唯一，而是存在于一个区间内。于
是，有：

定义 (线性支持向量机) ：对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题
[[eq:linearnondiv]] ，得到的分离超平面为：
  \begin{equation}
    \omega^{\ast} \cdot x + b^{\ast} = 0
  \end{equation}
以及相应的分类决策函数
  \begin{equation}
    f(x) = sign(\omega^{\ast} \cdot x + b^{\ast})
  \end{equation}
称为线性支持向量机。

** 学习的对偶算法
原始最优化问题的拉格朗日函数为：
  \begin{equation}
    \frac{1}{2}||\omega||^2 + C\sum^{N}_{i=1}\xi_i-\sum^{N}_{i=1}\alpha_i(
    y_i(w\cdot{}x_i+b)-1+\xi_i) - \sum^{N}_{i=1}\mu_i\xi_i
  \end{equation}
其中， $\alpha_i\geq{}0,\mu_i\geq{}0$ 。

利用 $KKT$ 条件， 即 $\nabla_{\omega}L(\omega,b,\xi,\alpha,\mu) = \nabla_b L = \nabla_\xi L = 0$ 和
不等式 $\alpha_i \geq 0, \mu_i \geq 0$ 可以推导出原始问题的对偶问题：
  \begin{eqnarray*}
    \min_{\alpha} && \frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
    -\sum^{N}_{i=1}\alpha_i\\
    s.t. && \sum^{N}_{i=1}\alpha_iy_i = 0\\
    && 0 \leq \alpha_i \leq C, i=1,2,\ldots,N
  \end{eqnarray*}

定理 5： 设 $\alpha^{\ast}$ 是对偶最优化问题的解，若存在下标 $j$ ，使得 $0 < \alpha_{j}^{\ast} < C$ ，
   并可按下式求得原始最优化问题的解 $\omega^{\ast}, b^{\ast}$ ：
     \begin{eqnarray}
       \omega^{\ast} &=& \sum^{N}_{i=1}\alpha_{i}^{\ast}y_ix_i\\\nonumber
       b^{\ast} &=& y_j - \sum^{N}_{i=1}\alpha^{\ast}y_i(x_i\cdot{}x_j)
     \end{eqnarray}

由此，有算法总结如下：

算法 3：线性支持向量机学习算法

   输入： 线性训练集 $T={(x_1,y_1), (x_2,y_2), \ldots, (x_N, y_N)}$ ，其中
   $x_i\in{}\mathcal{X}={\bm R}^n, y_i\in{}\mathcal{Y}={+1, -1}, i=1,2,\ldots,N$ ；

   输出：分离超平面和分类决策函数

   1) 选择惩罚参数 $C>0$ ，构造并求解凸二次规划问题：
        \begin{eqnarray*}
          \min_{\alpha}&&\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot{}x_j)
                         -\sum^N_{i=1}\alpha_i\\
          s.t. && \sum^N_{i=1}\alpha_iy_i = 0\\
                      && 0 \leq \alpha_i \geq C, i=1,2,\ldots,N
        \end{eqnarray*}
      求解最优解 $\alpha^{\ast} = (\alpha^{\ast}_1,\alpha^{\ast}_2,\ldots,\alpha^{\ast}_N)^T$

   2) 计算 $\omega^{\ast} = \sum^{N}_{i=1}\alpha^{\ast}_iy_ix_i$ ，选择 $\alpha^{\ast}$ 的一个分量
      $\alpha^{\ast}_j$ 适合条件 $0 < \alpha^{\ast}_j < C$ ，计算：
        \begin{equation*}
          b^{\ast} = y_j - \sum^{N}_{i=1}\alpha^{\ast}_iy_i(x_i\cdot{}x_j)
        \end{equation*}

   3) 求得分离超平面
        \begin{equation*}
          \omega^{\ast}\cdot{}x+b^{\ast} = 0
        \end{equation*}
      分离决策函数：
        \begin{equation*}
          f(x) = sign(\omega^{\ast}\cdot{}x+b^{\ast})
        \end{equation*}

   4) $b^{\ast}$ 取值的说明
      对于任一适合条件 $0 < \alpha^{\ast}_j < C$ 的 $\alpha^{\ast}_j$ ，都可以用来求解 $b^{\ast}$ ，
      因此，实际计算的适合，可以取所有符合条件的样本点上的平均。

** 支持向量
在线性不可分情况下，解 $\alpha^{\ast} = (\alpha^{\ast}_1,\alpha^{\ast}_2,\ldots,\alpha^{\ast}_N)^T$
中对应于 $\alpha^{\ast}_i > 0$ 的样本点 $(x_i,y_i)$ 的实例 $x_i$ 称为支持向量 (KKT 条件中的互补松弛条
件)。如下图所示，分离超平面由实线表示，间隔边界由虚线表示，正例点由 " $\circ$ " 表示，负例点由
" x " 表示，图中还标出了实例 $x_i$ 到间隔边界的距离 $\frac{1-\xi_i}{||\omega||}$ (图片标的有问题)
#+CAPTION: 软间隔支持向量
#+NAME: fig:softinterval
#+ATTR_LATEX: :width 8cm
[[./figs/supvector_04.png]]

软间隔支持向量 $x_i$ 或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分的一侧，
如果 $\alpha^{\ast} < C$ (对应 $\mu > 0$ ，考虑到互补松弛条件 $\mu^{\ast}_i\xi^{\ast}_i = 0$ )，那么
$\xi_i = 0$ ；若 $\alpha^{\ast}_i = C$ ， $0 < \xi_i < 1$ ，则分类正确， $x_i$ 在间隔边界与分离超平
面之间；若 $\alpha^{\ast}=C$ ， $\xi_i > 1$ ，则 $x_i$ 位于分离超平面误分一侧。

** 合页损失函数
线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：
  \begin{equation}
    \sum^{N}_{i=1}[1-y_i(\omega\cdot{}x_i+b)]_++\lambda||\omega||^2
  \end{equation}
目标函数第一项是经验损失或经验风险。函数
\begin{equation}
      L(y(\omega\cdot{}x+b)) = [1-y_i(\omega\cdot{}x+b)]_+
\end{equation}
称为合页损失函数 (hinge loss function)。下标 "+" 表示以下取正值的函数：
  \begin{equation}
    [z]_+ =
    \begin{cases}
      z & z > 0\\
      0 & z \leq 0
    \end{cases}
  \end{equation}
这就是说，当样本点 $(x_i, y_i)$ 被正确分类且函数间隔 (确信度) $y_i(\omega\cdot{}x_i+b)>1$ 时，损失是
0，否则损失为 $1-y_i(\omega\cdot{}x_i+b)$ 。

定理 6：线性支持向量机原始最优化问题：
  \begin{eqnarray*}
     \min_{\omega,b,\xi} && \frac{1}{2}||\omega||^2 + C\sum^{N}_{i=1}\xi_i\\
      s.t. && y_i(w\cdot{}x_i+b) \geq 1-\xi_i, i = 1,2,\ldots,N\\
      && \xi_i \geq 0, i = 1,2,\ldots,N
  \end{eqnarray*}
等价于最优化问题
  \begin{eqnarray*}
    \min_{\omega,b} && \sum^{N}_{i=1}[1-y_i(\omega\cdot{}x_i+b)]_++\lambda||\omega||^2
  \end{eqnarray*}

* 核函数

** 核技巧
如下图所示，通过非线性变换，将非线性问题转变为线性问题，这样的方法属于核技巧。

[[./figs/supvector_05.jpg]]

设原空间为 $\mathcal{X}\subset{}{\rm R}^2, x=(x_1,x_2)^T\in{}\mathcal{X}$ ，新空间为
$\mathcal{Z}\subset{}{\rm R}^2, z=(z_1, z_2)^T\in{}\mathcal{Z}$ ，定义从原空间到新空间的变换：
  \begin{equation*}
    z=\phi(x)=((x^{(1)})^2, (x^{(2)})^2)^T
  \end{equation*}
经过变换 $z=\phi(x)$ ，原空间 $\mathcal{X}\subset{}{\rm R}^2$ 变换为新空间
$\mathcal{Z}\subset{}{\rm R}^2$ ，原空间中的点相应地变换为新空间中的点，原空间中的椭圆
  \begin{equation*}
    \omega_1(x^{(1)})^2+\omega_2(x^{(2)})^2+b=0
  \end{equation*}
变换为新空间中的直线
  \begin{equation*}
      \omega_1z^{(1)} + \omega_2z^{(2)} + b = 0
  \end{equation*}
在新变换空间中，直线 $\omega_1z^{(1)} + \omega_2z^{(2)} + b = 0$ 将变换后的正负例点正确分开。这样，
原空间的非线性可分问题就转变为新空间中的线性可分问题。

总结上例，用线性方法求解非线性问题分为两步 (核技巧)：
+ 使用一个变换将原空间中的数据映射到新空间
+ 在新空间用线性可分学习方法从训练数据中学习分类模型

定义 (核函数)：设 $\mathcal{X}$ 是输入空间 (欧氏空间 ${\rm R}^n$ 的子集或离散集合)，又设
$\mathcal{H}$ 是特征空间 (希尔伯特空间)，如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射
  \begin{eqnarray*}
    \phi(x): && \mathcal{X} \to \mathcal{H}
  \end{eqnarray*}
使得对所有 $x,z \in \mathcal{X}$ ，函数 $K(x,z)$ 满足条件
  \begin{equation*}
    K(x,z) = \phi(x)\cdot{}\phi(z)
  \end{equation*}
则称 $K(x,z)$ 为核函数， $\phi(x)$ 为映射函数， 式中 $\phi(x)\cdot{}\phi(z)$ 为 $\phi(x)$ 和
$\phi(z)$ 的内积。

在线性支持向量机中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积。在对偶问题的目标函数
中，内积 $x_i\cdot{}x_j$ 可以用核函数 $K(x_i, x_j) = \phi(x_i)\cdot{}\phi(x_j)$ 代表，此时，对偶问题
的目标函数变为：
  \begin{equation*}
    W(\alpha) = \frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_jK(x_i, x_j)
                           -\sum^N_{i=1}\alpha_i
  \end{equation*}
同样，分类觉得函数中的内积也可以用核函数代替，于是，分类决策函数变为
  \begin{equation*}
    f(x) = sign(\sum^{N}_{i=1}\alpha^{\ast}y_i\phi(x_i)\cdot{}\phi(x)+b^{\ast})
    = sign(\sum^{N}_{i=1}\alpha_i^{\ast}y_iK(x_i, x)+b^{\ast})
  \end{equation*}

** 常用核函数                                          :not fully understand:
1. 多项式核函数 (polynomial kernel function)
    \begin{equation}
        K(x,z) = (x\cdot{}z+1)^p
    \end{equation}
   对应的支持向量机是一个 $p$ 次多项式分类器，在此情形下，分类决策函数成为：
      \begin{equation}
        f(x) = sign(\sum^{N_S}_{i=1}\alpha_i^{\ast}y_i(x_i\cdot{}x)^p+b^{\ast})
      \end{equation}
2. 高斯核函数 (Gaussian kernel function)
    \begin{equation}
      K(x,z) = exp\left( -\frac{||x-z||^2}{2\sigma^2} \right)
    \end{equation}
   对应的支持向量机是高斯径向基函数 (radial basis function) 分类器。在此情形下，分类决策函数成为
    \begin{equation}
      f(x) = \sum^{N_s}_{i=1}sign\left(\alpha_i^{\ast}y_iexp\left( -\frac{||x-z||^2}{2\sigma^2}
      \right)+b^{\ast}\right)
    \end{equation}
3. 字符串核函数
考虑一个有限字符表 $\Sigma$ 。字符串 s 是从 $\Sigma$ 中取出的有限个字符的序列，包括空字符串。字符串
s 的长度用 $|s|$ 表示，它的元素记做 $s(1)s(2)\ldots{}s(|s|)$ 。两个字符串的连接记做 $st$ ，所有长度
为 $n$ 的字符串的集合记做 $\Sigma^{n}$ ，所有字符串的集合记做
$\Sigma^{\ast}=\cup^{\infty}_{n=0}\Sigma^n$ 。

考虑字符串 $s$ 的字串 $u$ ，给定一个指标序列
$i=(i_1,i_2,\ldots,i_{|u}), 1 \leq i_1 < i_2 < \ldots < i_{|u|} \leq |s|$ ， $s$ 的字串定义为
$u=s(i)=s(i_1)s(i_2)\ldots{}s(i_{|u|})$ ，其长度记做 $l(i)=i_{|u|}-i_1+1$ 。如果 $i$ 是连续的，则
$l(i)=|u|$ ；否则， $l(i) > |u|$ 。

假设 $\mathcal{S}$ 是长度大于或等于 $n$ 的字符串集合， $s$ 是 $\mathcal{S}$ 的元素，现在建立字符串集
合 $S$ 到特征空间 $\mathcal{H}={\rm R}^{\Sigma^n}$ 的映射 $\phi_n(s)$ 。 ${\rm R}^{\Sigma^n}$ 表示定
义在 $\Sigma^n$ 上的实数空间，其每一维对应一个字符串 $u\in{}\Sigma^n$ ，映射 $\phi_n(s)$ 将字符串
$s$ 对应于空间 $R^{\Sigma^n}$ 的一个向量，其在 $u$ 上的取值为：
  \begin{equation}
    [\phi_n(s)]_u=\sum_{i;s(i)=u}\lambda^{l(i)}
  \end{equation}
这里， $ 0 < \lambda \leq 1$ 表示一个衰减参数， $l(i)$ 表示字符串 $i$ 的长度，求和在 $s$ 中所有与
$u$ 相同的字串上进行。

譬如，假设 $\Sigma$ 是应为字符集， $n$ 为 3， $\mathcal{S}$ 为长度大于等于 3 的字符串集合。考虑将字符
集 $\mathcal{S}$ 映射到特征空间 $\mathcal{H}_3$ 。 $\mathcal{H}_3$ 的一维对应字符串 $asd$ 。这时，字
符串 “Nasdaq” 与 “lass das” 在这一维上的值分别是 $[\phi_3(Nasdaq)]_{asd}=\lambda^3$ 和
$[\phi_3(lass das)]_{asd}=2\lambda^5$ ，在第一个字符串里， $asd$ 是连续的字串，第二个字符串里，
$asd$ 是长度为 5 的不连续字串，共出现 2 次。两个字符串 $s$ 和 $t$ 上的字符串核函数是基于映射 $\phi_n$ 的
特征空间中的内积：
  \begin{equation}
   k_n(s,t) = \sum_{u\in{}\Sigma^n}[\phi_n(s)]_{u}[\phi_n(t)]_{u} =
   \sum_{u\in{}\Sigma^n(i,j):}\sum_{s(i)=t(j)=u} \lambda^{l(i)}\lambda^{l(j)}
  \end{equation}
字符串核函数 $k_n(s,t)$ 给出了字符串 $s$ 和 $t$ 中长度等于 $n$ 的所有字串组成的特征向量的余弦相似度
(cosine similarity) 。直观上，两个字符串相同的字串越多，他们越相似，字符串核函数的值就越大。字符串核
函数可以由动态规划快速计算。

** 非线性支持向量机
定义 (非线性支持向量机)：从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划，学习得到的分类
决策函数：
  \begin{equation}
    f(x) = sign(\sum^{N}_{i=1}\alpha_i^{\ast}y_iK(x_i,x)^p+b^{\ast})
  \end{equation}
称为非线性支持向量， $K(x,z)$ 是正定核函数[fn:7]。

算法 4： 非线性支持向量机学习算法

   输入： 训练数据集 $T={(x_1,y_1), (x_2,y_2), \ldots, (x_N, y_N)}$ ，其中
   $x_i\in{}\mathcal{X}={\bm R}^n, y_i\in{}\mathcal{Y}={+1, -1}, i=1,2,\ldots,N$ ；

   输出：分类决策函数

   1) 选择合适的核函数 $K(x,z)$ 和适当的参数 $C$ ，构造并求解最优化问题：
        \begin{eqnarray*}
          \min_{\alpha}&&\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_jK(x_i,x_j)
                         -\sum^N_{i=1}\alpha_i\\
          s.t. && \sum^N_{i=1}\alpha_iy_i = 0\\
                      && 0 \leq \alpha_i \leq C, i=1,2,\ldots,N
        \end{eqnarray*}
      求解最优解 $\alpha^{\ast} = (\alpha^{\ast}_1,\alpha^{\ast}_2,\ldots,\alpha^{\ast}_N)^T$

   2) 选择 $\alpha^{\ast}$ 的一个正分量 $0 < \alpha^{\ast}_j < C$ ，计算：
        \begin{equation*}
          b^{\ast} = y_j - \sum^{N}_{i=1}\alpha^{\ast}_iy_iK(x_i,x_j)
        \end{equation*}

   3) 构造决策函数
        \begin{equation*}
          f(x) = sign(\sum^N_{i=1}\alpha^{\ast}_iy_iK(x\cdot{}x_i)+b^{\ast})
        \end{equation*}
      当 $K(x,z)$ 是正定核时，上述问题是凸二次规划问题，解存在。

* 序列最小最优化算法
序列最小最优化算法 (=SMO=) 要解如下二次规划的对偶问题：
#+NAME: eq:smosolve
  \begin{eqnarray*}
    \min_{\alpha}&&\frac{1}{2}\sum^{N}_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jy_iy_jK(x_i,x_j)
                    -\sum^N_{i=1}\alpha_i\\
    s.t. && \sum^N_{i=1}\alpha_iy_i = 0\\
                 && 0 \leq \alpha_i \leq C, i=1,2,\ldots,N
  \end{eqnarray*}
在这个问题中，变量是拉格朗日乘子，一个变量 $\alpha_i$ 对应一个样本点 $(x_i, y_i)$ ；变量的总数等于训
练样本容量 $N$ 。

=SMO= 算法是一种启发式算法，其基本思路是：如果所有变量的解都满足此优化问题的 $KKT$ 条件，那么这个优
化问题的解就得到了。因为 $KKT$ 条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，
针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解
，因为这会使得原始二次规划问题的目标函数变得更小，重要的是，这时，子问题可以通过解析方法求解，这样可
以大大提高整个算法的计算速度。子问题有两个变量，一个是违反 $KKT$ 条件最严重的那一个，另一个由约束条
件自动确定。这样， =SMO= 算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。

需要注意的是，子问题的两个变量中只有一个是自由变量，假设 $\alpha_1,\alpha_2$ 为两个变量，
$\alpha_3, \alpha_4,\ldots,\alpha_N$ 固定，那么由等式约束 $\sum^N_{i=1}\alpha_iy_i = 0$ 可知，
  \begin{equation*}
    \alpha_1 = -y_1\sum^{N}_{i=2}\alpha_iy_i
  \end{equation*}
于是，如果 $\alpha_1$ 固定的话， $\alpha_2$ 也就固定了。

** 两个变量二次规划的求解方法
不失一般性，假设选择的两个变量是 $\alpha_1,\alpha_2$ ，其他变量 $\alpha_i(i=3,4,\ldots,N)$ 是固定的
，于是 =SMO= 的最优化问题 [[eq:smosolve]] 的子问题可以写成：
#+NAME: eq:smoeg
  \begin{eqnarray*}
    \min_{\alpha_1, \alpha_2}&&\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2
                   + y_1y_2K_{12}\alpha_1\alpha_2\\\nonumber
    {}&&-(\alpha_1+\alpha_2)+y_1\alpha_1\sum^{N}_{i=3}y_i\alpha_iK_{i1}+
         y_2\alpha_2\sum^{N}_{i=3}y_i\alpha_iK_{i2}\\
    s.t. && \alpha_1y_1 + \alpha_2y_2 = -\sum^{N}_{i=3}y_i\alpha_i=\varsigma\\
                 && 0 \leq \alpha_i \leq C, i=1,2
  \end{eqnarray*}

1. 确定约束
   不等式约束 $0 \leq \alpha_i \leq C$ 使得 $\alpha_i,i=1,2$ 只能在盒子 $[0,C]\times{}[0,C]$ 范围内，等式约束
   $\alpha_1y_1 + \alpha_2y_2 =\varsigma$ 使得 $\alpha_i,i=1,2$ 只能在斜线段
   $\alpha_1+\alpha_2 = \varsigma$ 或 $\alpha_1-\alpha_2 = \varsigma$ 上取值。
   下图是 $\alpha_1, \alpha_2$ 异号时，约束的表示
   [[./figs/supvector_06.png]]
   从图中可以看出，如果问题 [[eq:smoeg]] 的初始可行解为 $\alpha^{old}_1, \alpha^{old}_2$ ，最优解为
   $\alpha^{new}_1, \alpha^{new}_2$ ，那么 $\alpha^{new}_2$ 取值范围必须满足：
     \begin{equation}
       L \leq \alpha^{new}_2 \leq H
     \end{equation}
   其中， $L=max(0, \alpha^{old}_2-\alpha^{old}_1, H=min(C,C+\alpha^{old}_2-\alpha^{old}_1)$ ，如果
   $y_1 = y_2$ ，类似可以得到 $\alpha^{new}_2$ 的取值约束为：
   $L=max(0, \alpha^{old}_2+\alpha^{old}_1-C, H=min(C,\alpha^{old}_2+\alpha^{old}_1)$ 。
2. 求沿着约束方向未经剪辑的 $\alpha^{new,unc}_2$
3. 剪辑后 $\alpha^{new}_2$
* Footnotes

[fn:7] 设 $\mathcal{X}\subset{}{\bm R}^n$ ， $K(x,z)$ 是定义在 $\mathcal{X}\times{}\mathcal{X}$ 上的
对称函数，如果对任意 $x_i\in \mathcal{X}, i=1,2,\ldots,m$ ， $K(x,z)$ 对应的 Gram 矩阵：
  \begin{equation*}
    K=[K(x_i,x_j)]_{m\times{}m}
  \end{equation*}
是半正定矩阵，则称 $K(x,z)$ 是正定核。

[fn:6] 利用 KKT 条件， $\nabla_{\omega} L(\omega^{\ast}, b^{\ast}, \alpha^{\ast}) = \nabla_{b} L = 0$
可以求出 $\omega^{\ast}$ ，代入 $\alpha_{i}^{\ast}(y_i(\omega^{\ast}\cdot{}x_i+b^{\ast})-1=0$
可以求出 $b^{\ast}$ 。

[fn:5] 简单而言，仿射函数是指一阶多项式组成的函数，譬如 $y = ax+b$ 。

[fn:4] 简单从理解上讲， $f^{\prime\prime}(x) \geq 0$ 对应凸函数， $f^{\prime\prime}(x) < 0$ 对应凹函数。

[fn:3] 最大化 $\frac{1}{|x|}$ 等价于最小化 $|x|$ 等价于最小化 $\frac{1}{2}|x|^2$ 。

[fn:2] 事实上，如果考虑到拉格朗日乘子法的时候，这一点可以更加明显地表现出来。

[fn:1] 线性可分的定义：给定一个数据集 $T = \{(x_1,y_1), (x_2,y_2), \ldots, (x_N,y_N)\}$ ，其中，
$x_i \in \mathcal{X} = {\bm R}^n, y_i \in \mathcal{Y} = \{+1, -1\}, i=1,2,\ldots,N$ ，如果存在某个超
平面 S： $\omega \cdot x + b = 0$ 能够将数据集完全正确地划分到超平面的两侧，即对所有 $y_i = +1$ 的实
例 $i$ ，有 $\omega \cdot x + b > 0$ ，对所有 $y_i = -1$ 的实例 $i$ ，有 $\omega \cdot x + b < 0$ ，
则称数据集 T 是线性可分数据集 (linear separable data set)；否则，称数据集为线性不可分。
